# Building-A-Neural-Network-from-Scratch
The task is the implementation of the Adam optimizer for training feed-forward neural  networks using the Back-Propagation (BP) method by automatically computing 
the gradient  of the loss function. The implemented binary classification - regression model includes several  activation functions, regularization methods for 
avoiding overfitting (L2 and Dropout),  optimization algorithms (Gradient Descent and Adam optimizer), as well as grid search that  serves hyperparameter tuning 
purposes. The full model pipeline is tested on several datasets.  
The project is completely built from scratch.

Prerequisites
----------------------
Running this project on the local system requires the following packages to be installed :

- numpy
- matplotlib
- pandas
- time
- scipy
- seaborn
- sklearn
- imblearn
- warnings

Instructions on How to Run script
---------------------------------
The user can also use Google Colab in a Web Browser without needing to install the mentioned packages.

This project is implemented as a python script. The user just need to install the mentioned-above libraries on his/her local system or on Google Colab and execute main.py. 
Note that the dataset for the training and validation data should be placed in the same directory as the python scripts.
The function of python script is properly explained with the help of comments.

The user can change the parameters of the Neural Network Algorithm from the config.py

Executing the main.py will automatically start the Neural Network Model training.
